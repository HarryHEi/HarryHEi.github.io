<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Ben Wong,wenbinben@gmail.com"><title>scrapy_harry · Harryx</title><meta name="description" content="创建新项目需要创建一个项目，创建一个名为project的项目
1scrapy startproject project

创建的项目结构如下
1234567891011tree project -L 3project/  ├── project/ #项目的Python模块，在此加入代码 │   ├─"><meta name="keywords" content="Hexo,HTML,Ben,CSS,安卓,android,Linux,linuxdeepin"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">Harryx</a></h3><div class="description"><p>good good study good good play~</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/harryx520"><i class="fa fa-twitter"></i></a></li><li><a href="http://weibo.com/Booooyakasha"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/avator.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>scrapy_harry</a></h3></div><div class="post-content"><h1 id="创建新项目"><a href="#创建新项目" class="headerlink" title="创建新项目"></a>创建新项目</h1><p>需要创建一个项目，创建一个名为project的项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject project</span><br></pre></td></tr></table></figure>

<p>创建的项目结构如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tree project -L 3</span><br><span class="line"></span><br><span class="line">project/  </span><br><span class="line">├── project/ #项目的Python模块，在此加入代码 </span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── items.py</span><br><span class="line">│   ├── pipelines.py</span><br><span class="line">│   ├── settings.py #项目的设置文件，headers等等都在里面配置</span><br><span class="line">│   └── spiders/ #放置spider代码的目录</span><br><span class="line">│       └── __init__.py</span><br><span class="line">└── scrapy.cfg  #配置文件</span><br></pre></td></tr></table></figure>

<h1 id="定义Item"><a href="#定义Item" class="headerlink" title="定义Item"></a>定义Item</h1><p>继承<code>scrapy.Item</code>类</p>
<p><code>Item.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class FirstItem(scrapy.Item):</span><br><span class="line">    title = scrapy.Field()</span><br><span class="line">    link = scrapy.Field()</span><br><span class="line">    des = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h1 id="编写第一个爬虫"><a href="#编写第一个爬虫" class="headerlink" title="编写第一个爬虫"></a>编写第一个爬虫</h1><p>继承<code>scrapy.Spider</code>类</p>
<ul>
<li><code>name</code>是爬虫的名字</li>
<li><code>start_urls</code>爬虫爬取的URLS</li>
<li><code>parse()</code>爬取方法，每个初始URL完成下载后生成，<code>Response</code>对象作为唯一参数传递给该函数，该方法负责解析数据，提取数据，生成<code>item</code>和进一步要处理的URL的<code>Request</code></li>
</ul>
<p><code>spiders/first_spider.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class FirstSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;first&quot;</span><br><span class="line">    base_domain = &quot;harryhei.github.io&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &quot;http://harryhei.github.io/tags/flask/&quot;,</span><br><span class="line">        &quot;http://harryhei.github.io/tags/django/&quot;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        f = response.url.split(&quot;/&quot;)[-2]</span><br><span class="line">        with open(f, &quot;wb&quot;) as f:</span><br><span class="line">            f.write(response.body)</span><br></pre></td></tr></table></figure>

<p>制定headers和cookies参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Request(url, method=&apos;GET&apos;, headers=headers, cookies=cookies)</span><br></pre></td></tr></table></figure>

<h1 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h1><p>在根目录project下使用命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy list #列出了所有spider</span><br><span class="line">scrapy crawl first #启动了名为first的爬虫</span><br></pre></td></tr></table></figure>

<p>爬完之后在根目录下多了两个文件：<code>django</code>和<code>flask</code></p>
<p>Scrapy为每个URL创建了 <code>scrapy.Request</code> 对象，并将 <code>parse</code> 方法作为回调函数(callback)赋值给了<code>Request</code>。</p>
<p><code>Request</code>对象经过调度，执行生成 <code>scrapy.http.Response</code> 对象并送回给 <code>parse()</code> 方法。</p>
<h1 id="提取Item"><a href="#提取Item" class="headerlink" title="提取Item"></a>提取Item</h1><p>selectors选择器：基于XPath和CSS<br><a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/selectors.html#scrapy.selector.Selector.xpath" target="_blank" rel="noopener">选择器</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.selector import Selector</span><br><span class="line">from scrapy.http import HtmlResponse</span><br><span class="line"></span><br><span class="line">body = &apos;...&apos;</span><br><span class="line">Selector(text=body).xpath(&apos;//span/text()&apos;).extract()</span><br></pre></td></tr></table></figure>

<p>XPath举例</p>
<ul>
<li><code>/html/head/title</code> ：选择HTML文档中<code>&lt;head&gt;</code>标签内的<code>&lt;title&gt;</code>元素</li>
<li><code>/html/head/title/text()</code>：选择<code>&lt;title&gt;</code>元素的文字</li>
<li><code>//td</code>：所有<code>&lt;td&gt;</code>元素</li>
<li><code>//div[@class=&quot;mine&quot;]</code>：所有具有<code>class=&quot;mine&quot;</code>属性的<code>div</code>元素</li>
</ul>
<p><a href="http://www.w3school.com.cn/xpath/xpath_syntax.asp" target="_blank" rel="noopener">xpath语法</a></p>
<p>Selector基本方法</p>
<ul>
<li><code>xpath()</code>：传入xpath表达式，返回selector list</li>
<li><code>css()</code>：传入CSS表达式，返回selector list</li>
<li><code>extract()</code>：返回unicode list</li>
<li><code>re()</code>：传入正则表达式，返回unicode list</li>
</ul>
<p>Scrapy shell</p>
<p>浏览器配合分析元素，抓取内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell &quot;http://harryhei.github.io&quot; //获得一个response对象</span><br><span class="line"></span><br><span class="line">response.xpath(&quot;//p[@class=&apos;article-more-link&apos;]/a/@href&quot;) //所有链接</span><br><span class="line">response.xpath(&quot;//a[@class=&apos;article-title&apos;]/text()&quot;).extract() //所有标题</span><br></pre></td></tr></table></figure>

<p>写到项目文件中</p>
<p>输出爬到的标题、链接和描述</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class FirstSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;first&quot;</span><br><span class="line">    base_domain = &quot;harryhei.github.io&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &quot;http://harryhei.github.io&quot;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for sel in response.xpath(&quot;//div[@class=&apos;body-wrap&apos;]/article&quot;):</span><br><span class="line">            title = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//a/text()&quot;).extract()</span><br><span class="line">            link = sel.xpath(&quot;div[@class=&apos;article-meta&apos;]//a/@href&quot;).extract()</span><br><span class="line">            des = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//ul/li/a/text()&quot;).extract()</span><br><span class="line">            print title[0],link[0]</span><br><span class="line">            for l in des:</span><br><span class="line">                print &quot;&gt;&quot;,l</span><br></pre></td></tr></table></figure>

<p>使用item</p>
<p><code>response.urljoin()</code>用于构造绝对路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">from project.items import FirstItem</span><br><span class="line"></span><br><span class="line">class FirstSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;first&quot;</span><br><span class="line">    base_domain = &quot;harryhei.github.io&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &quot;http://harryhei.github.io&quot;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for sel in response.xpath(&quot;//div[@class=&apos;body-wrap&apos;]/article&quot;):</span><br><span class="line">            item = FirstItem()</span><br><span class="line">            title = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//a/text()&quot;).extract()</span><br><span class="line">            link = sel.xpath(&quot;div[@class=&apos;article-meta&apos;]//a/@href&quot;).extract()</span><br><span class="line">            des = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//ul/li/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;title&quot;] = title[0]</span><br><span class="line">            item[&quot;link&quot;] = response.urljoin(link[0]) #构造绝对路径</span><br><span class="line">            item[&quot;des&quot;] = &quot; &quot;.join(des)</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<p>追踪链接</p>
<p>使用parse找寻链接，然后调用回调函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">from project.items import FirstItem</span><br><span class="line"></span><br><span class="line">class FirstSpider(scrapy.Spider):</span><br><span class="line">    name = &quot;first&quot;</span><br><span class="line">    base_domain = &quot;harryhei.github.io&quot;</span><br><span class="line">    start_urls = [</span><br><span class="line">        &quot;http://harryhei.github.io&quot;</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        for i in range(10):</span><br><span class="line">            url = response.urljoin(&quot;/page/%s&quot;%(i+1))</span><br><span class="line">            yield scrapy.Request(url, callback=self.parse_dir_contents)</span><br><span class="line"></span><br><span class="line">    def parse_dir_contents(self, response):</span><br><span class="line">        for sel in response.xpath(&quot;//div[@class=&apos;body-wrap&apos;]/article&quot;):</span><br><span class="line">            item = FirstItem()</span><br><span class="line">            title = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//a/text()&quot;).extract()</span><br><span class="line">            link = sel.xpath(&quot;div[@class=&apos;article-meta&apos;]//a/@href&quot;).extract()</span><br><span class="line">            des = sel.xpath(&quot;div[@class=&apos;article-inner&apos;]//ul/li/a/text()&quot;).extract()</span><br><span class="line">            item[&quot;title&quot;] = title[0]</span><br><span class="line">            item[&quot;link&quot;] = response.urljoin(link[0])</span><br><span class="line">            item[&quot;des&quot;] = &quot; &quot;.join(des)</span><br><span class="line">            yield item</span><br></pre></td></tr></table></figure>

<h1 id="保存数据"><a href="#保存数据" class="headerlink" title="保存数据"></a>保存数据</h1><p>保存为json数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl first -o items.json</span><br></pre></td></tr></table></figure>

<p>Python 读取json数据然后输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">f = open(&quot;items.json&quot;)</span><br><span class="line">j = json.loads(f.read())</span><br><span class="line">for l in j:</span><br><span class="line">    print l[&quot;title&quot;], l[&quot;link&quot;]</span><br><span class="line">    print l[&quot;des&quot;]</span><br><span class="line">    </span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>

<h1 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h1><p><a href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/index.html" target="_blank" rel="noopener">中文文档</a></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2016-10-02</span><i class="fa fa-tag"></i><a class="tag" href="/tags/spider/" title="spider">spider </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2016/10/02/Note/python/library/spider/scrapy/,Harryx,scrapy_harry,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2016/10/18/Note/cpp/cpp_primer_part2/" title="C++ Primer">上一篇</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2016/09/29/Note/python/demo/wxbot/" title="wxBot微信机器人框架">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>