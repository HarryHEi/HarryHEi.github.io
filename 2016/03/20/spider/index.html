<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Ben Wong,wenbinben@gmail.com"><title>爬图 · Harryx</title><meta name="description" content="功能此程序通过urllib2等模块，爬取网页妹子图片
爬妹子图片1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636"><meta name="keywords" content="Hexo,HTML,Ben,CSS,安卓,android,Linux,linuxdeepin"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Harryx</a></h3><div class="description"><p>good good study good good play~</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/harryx520"><i class="fa fa-twitter"></i></a></li><li><a href="http://weibo.com/Booooyakasha"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="http://7xrooc.com1.z0.glb.clouddn.com/avatar.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>爬图</a></h3></div><div class="post-content"><h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>此程序通过urllib2等模块，爬取网页妹子图片</p>
<h1 id="爬妹子图片"><a href="#爬妹子图片" class="headerlink" title="爬妹子图片"></a>爬妹子图片</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># -*- coding: utf-8 -*-  </div><div class="line"></div><div class="line">import threading,urllib2,urllib</div><div class="line">import os,re,string,sys,time</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">&quot;抓图&quot;</div><div class="line"></div><div class="line">countnum = 0</div><div class="line">obj = &quot;/home/herui/Python_Spider/&quot;</div><div class="line">threads = []</div><div class="line"></div><div class="line"># 获取页面的html</div><div class="line">def get_page(url):</div><div class="line">        req = urllib2.Request(url)</div><div class="line">        try:</div><div class="line">                html = urllib2.urlopen(req)</div><div class="line">                return html</div><div class="line">        except urllib2.URLError, e:</div><div class="line">            if hasattr(e, &quot;code&quot;):</div><div class="line">                print &quot;HTTP error: &quot; + str(e.code)</div><div class="line">            elif hasattr(e, &quot;reason&quot;):</div><div class="line">                print e.reason</div><div class="line"></div><div class="line">#获取分类链接</div><div class="line">def get_fenlei_link(url, html):</div><div class="line">    soup = BeautifulSoup(html, &apos;lxml&apos;)</div><div class="line">    links = soup.find_all(href = re.compile(&quot;/mm/&quot;))[0:5]</div><div class="line">    fenlei_links = []</div><div class="line">    for link in links:</div><div class="line">        longlink = str(link).split(&apos;\&quot;&apos;)[1]</div><div class="line">        fenlei_links.append(url+longlink)</div><div class="line">    return fenlei_links</div><div class="line"></div><div class="line">#获得页面所有图片的链接</div><div class="line">def get_pic_links(url):</div><div class="line">    req = urllib2.Request(url)</div><div class="line">    try:</div><div class="line">        html = urllib2.urlopen(req)</div><div class="line">    except urllib2.URLError, e:</div><div class="line">        if hasattr(e, &quot;code&quot;):</div><div class="line">            print &quot;HTTP error: &quot; + str(e.code)</div><div class="line">        elif hasattr(e, &quot;reason&quot;):</div><div class="line">            print e.reason</div><div class="line">    soup = BeautifulSoup(html, &apos;lxml&apos;)</div><div class="line">    links = soup.find_all(src = re.compile(&quot;.jpg&quot;))</div><div class="line">    pic_url = []</div><div class="line">    for link in links:</div><div class="line">        pic_url.append(re.findall(&quot;http:.*?.jpg&quot;,str(link)))</div><div class="line">    return pic_url</div><div class="line"></div><div class="line"></div><div class="line">#获得每个图包的链接</div><div class="line"># &quot;/mm/xxxxx/xxxxxxxxxxxxxx.html&quot;</div><div class="line">def get_pic_pack_link(page_index):</div><div class="line">    html = urllib2.urlopen(page_index).read()</div><div class="line">    links = re.findall(&quot;/mm/.*?html&quot;, str(html))[1:]</div><div class="line">    return links</div><div class="line"></div><div class="line">#抓取一个图包里面的十个图片</div><div class="line">def get_pack_pic(page_index,index_num, path, packlink ,pack_num):</div><div class="line">    global countnum</div><div class="line">    global obj</div><div class="line">    print &quot;开始下载第%d个图包&quot;%pack_num</div><div class="line">    for i in range(1,11): #每个图抱抓10个</div><div class="line">        name = obj+str(path)+&apos;/&apos;+str(index_num)+str(pack_num)+str(string.zfill(i,5))+&quot;.jpg&quot;</div><div class="line">        if i &lt;= 1:</div><div class="line">            packurl = str(re.findall(&quot;http://.*/&quot;, str(page_index))[0]) + str(str(str(packlink).split(&quot;.&quot;)[0]).split(&quot;/&quot;)[-1]) + &quot;.html&quot;</div><div class="line">        else:</div><div class="line">            packurl = str(re.findall(&quot;http://.*/&quot;, str(page_index))[0]) + str(str(str(packlink).split(&quot;.&quot;)[0]).split(&quot;/&quot;)[-1]) + &quot;-&quot; +str(i) + &quot;.html&quot;</div><div class="line">        try:</div><div class="line">            packhtml = urllib2.urlopen(packurl).read()</div><div class="line">            packpicurl = str(str(re.findall(&quot;http://22mm-img.*?_640.jpg&quot;, packhtml)).split(&quot;=&quot;)[-1]).split(&quot;,&quot;)[-1]</div><div class="line">            packpicurl = re.findall(&quot;http://.*?.jpg&quot;, packpicurl)[0]</div><div class="line">            packpicurl = packpicurl.replace(&quot;big&quot;,&quot;pic&quot;,1)</div><div class="line">        except:</div><div class="line">            continue</div><div class="line">        try:</div><div class="line">            urllib.urlretrieve(packpicurl,name)</div><div class="line">        except:</div><div class="line">            print &quot;图片下载失败&quot;</div><div class="line">            continue</div><div class="line">        countnum += 1</div><div class="line">        print &quot;%s&quot;%str(name) + &quot;下载成功&quot;</div><div class="line"></div><div class="line">#开始</div><div class="line">def start():</div><div class="line">        print &apos;Waiting............&apos;</div><div class="line">        url=&apos;http://www.22mm.cc&apos;</div><div class="line">        html=get_page(url)</div><div class="line">        fenlei_links = get_fenlei_link(url, html)</div><div class="line">        for link in fenlei_links:</div><div class="line">            pagenum = 0</div><div class="line">            while pagenum &lt;= 5:</div><div class="line">                pagenum += 1</div><div class="line">                if pagenum == 1:</div><div class="line">                    path=unicode(str(link).split(&quot;/&quot;)[-2],&apos;utf8&apos;)</div><div class="line">                    if not os.path.exists(path):</div><div class="line">                        os.mkdir(path)</div><div class="line">                    page_index = link+&quot;index&quot;+&quot;.html&quot;</div><div class="line">                else:</div><div class="line">                    page_index = link+&quot;index_&quot;+str(pagenum)+&quot;.html&quot;</div><div class="line">                packlinks = get_pic_pack_link(page_index) #每个分类每页每个图包首地址</div><div class="line">                pack_num = 0</div><div class="line">                for packlink in packlinks:</div><div class="line">                    thread = threading.Thread(target=get_pack_pic,args=(page_index, pagenum, path, packlink, pack_num))</div><div class="line">                    thread.setDaemon(True)</div><div class="line">                    thread.start()</div><div class="line">                    threads.append(thread)</div><div class="line">                    pack_num += 1</div><div class="line">                    print &quot;开始第%s个线程&quot;%pack_num</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    try:</div><div class="line">        start()</div><div class="line">        for thread in threads:</div><div class="line">            time.sleep(1)</div><div class="line">            while thread.join():</div><div class="line">                pass</div><div class="line">        print &quot;一共下载了%d个图片&quot;%countnum</div><div class="line">    except KeyboardInterrupt, e :</div><div class="line">        print &quot;一共下载了%d个图片&quot;%countnum</div><div class="line">        for thread in threads:</div><div class="line">            sys.exit(1)</div></pre></td></tr></table></figure></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2016-03-20</span><i class="fa fa-tag"></i><a href="/tags/spider/" title="spider" class="tag">spider </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/2016/03/20/spider/,Harryx,爬图,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2016/03/23/chatroom/" title="聊天室" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2016/03/19/threading/" title="多线程" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>