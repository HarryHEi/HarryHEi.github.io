<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Ben Wong,wenbinben@gmail.com"><title>爬图 · Harryx</title><meta name="description" content="功能此程序通过urllib2等模块，爬取网页妹子图片
爬妹子图片1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636"><meta name="keywords" content="Hexo,HTML,Ben,CSS,安卓,android,Linux,linuxdeepin"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title><a href="/">Harryx</a></h3><div class="description"><p>good good study good good play~</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/harryx520"><i class="fa fa-twitter"></i></a></li><li><a href="http://weibo.com/Booooyakasha"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">Home</a></li><li><a href="/about">Sobre</a></li><li><a href="/archives">Arquivo</a></li><li><a href="/links">Links</a></li></div><div class="information"><div class="back_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"> </a></li></div><div class="avatar"><img src="/images/avator.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>爬图</a></h3></div><div class="post-content"><h1 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h1><p>此程序通过urllib2等模块，爬取网页妹子图片</p>
<h1 id="爬妹子图片"><a href="#爬妹子图片" class="headerlink" title="爬妹子图片"></a>爬妹子图片</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding: utf-8 -*-  </span><br><span class="line"></span><br><span class="line">import threading,urllib2,urllib</span><br><span class="line">import os,re,string,sys,time</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line"></span><br><span class="line">&quot;抓图&quot;</span><br><span class="line"></span><br><span class="line">countnum = 0</span><br><span class="line">obj = &quot;/home/herui/Python_Spider/&quot;</span><br><span class="line">threads = []</span><br><span class="line"></span><br><span class="line"># 获取页面的html</span><br><span class="line">def get_page(url):</span><br><span class="line">        req = urllib2.Request(url)</span><br><span class="line">        try:</span><br><span class="line">            html = urllib2.urlopen(req)</span><br><span class="line">            return html</span><br><span class="line">        except urllib2.URLError, e:</span><br><span class="line">            if hasattr(e, &quot;code&quot;):</span><br><span class="line">                print &quot;HTTP error: &quot; + str(e.code)</span><br><span class="line">            elif hasattr(e, &quot;reason&quot;):</span><br><span class="line">                print e.reason</span><br><span class="line"></span><br><span class="line">#获取分类链接</span><br><span class="line">def get_fenlei_link(url, html):</span><br><span class="line">    soup = BeautifulSoup(html, &apos;lxml&apos;)</span><br><span class="line">    links = soup.find_all(href = re.compile(&quot;/mm/&quot;))[0:5]</span><br><span class="line">    fenlei_links = []</span><br><span class="line">    for link in links:</span><br><span class="line">        longlink = str(link).split(&apos;\&quot;&apos;)[1]</span><br><span class="line">        fenlei_links.append(url+longlink)</span><br><span class="line">    return fenlei_links</span><br><span class="line"></span><br><span class="line">#获得页面所有图片的链接</span><br><span class="line">def get_pic_links(url):</span><br><span class="line">    req = urllib2.Request(url)</span><br><span class="line">    try:</span><br><span class="line">        html = urllib2.urlopen(req)</span><br><span class="line">    except urllib2.URLError, e:</span><br><span class="line">        if hasattr(e, &quot;code&quot;):</span><br><span class="line">            print &quot;HTTP error: &quot; + str(e.code)</span><br><span class="line">        elif hasattr(e, &quot;reason&quot;):</span><br><span class="line">            print e.reason</span><br><span class="line">    soup = BeautifulSoup(html, &apos;lxml&apos;)</span><br><span class="line">    links = soup.find_all(src = re.compile(&quot;.jpg&quot;))</span><br><span class="line">    pic_url = []</span><br><span class="line">    for link in links:</span><br><span class="line">        pic_url.append(re.findall(&quot;http:.*?.jpg&quot;,str(link)))</span><br><span class="line">    return pic_url</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#获得每个图包的链接</span><br><span class="line"># &quot;/mm/xxxxx/xxxxxxxxxxxxxx.html&quot;</span><br><span class="line">def get_pic_pack_link(page_index):</span><br><span class="line">    html = urllib2.urlopen(page_index).read()</span><br><span class="line">    links = re.findall(&quot;/mm/.*?html&quot;, str(html))[1:]</span><br><span class="line">    return links</span><br><span class="line"></span><br><span class="line">#抓取一个图包里面的十个图片</span><br><span class="line">def get_pack_pic(page_index,index_num, path, packlink ,pack_num):</span><br><span class="line">    global countnum</span><br><span class="line">    global obj</span><br><span class="line">    print &quot;开始下载第%d个图包&quot;%pack_num</span><br><span class="line">    for i in range(1,11): #每个图抱抓10个</span><br><span class="line">        name = obj+str(path)+&apos;/&apos;+str(index_num)+str(pack_num)+str(string.zfill(i,5))+&quot;.jpg&quot;</span><br><span class="line">        if i &lt;= 1:</span><br><span class="line">            packurl = str(re.findall(&quot;http://.*/&quot;, str(page_index))[0]) + str(str(str(packlink).split(&quot;.&quot;)[0]).split(&quot;/&quot;)[-1]) + &quot;.html&quot;</span><br><span class="line">        else:</span><br><span class="line">            packurl = str(re.findall(&quot;http://.*/&quot;, str(page_index))[0]) + str(str(str(packlink).split(&quot;.&quot;)[0]).split(&quot;/&quot;)[-1]) + &quot;-&quot; +str(i) + &quot;.html&quot;</span><br><span class="line">        try:</span><br><span class="line">            packhtml = urllib2.urlopen(packurl).read()</span><br><span class="line">            packpicurl = str(str(re.findall(&quot;http://22mm-img.*?_640.jpg&quot;, packhtml)).split(&quot;=&quot;)[-1]).split(&quot;,&quot;)[-1]</span><br><span class="line">            packpicurl = re.findall(&quot;http://.*?.jpg&quot;, packpicurl)[0]</span><br><span class="line">            packpicurl = packpicurl.replace(&quot;big&quot;,&quot;pic&quot;,1)</span><br><span class="line">        except:</span><br><span class="line">            continue</span><br><span class="line">        try:</span><br><span class="line">            urllib.urlretrieve(packpicurl,name)</span><br><span class="line">        except:</span><br><span class="line">            print &quot;图片下载失败&quot;</span><br><span class="line">            continue</span><br><span class="line">        countnum += 1</span><br><span class="line">        print &quot;%s&quot;%str(name) + &quot;下载成功&quot;</span><br><span class="line"></span><br><span class="line">#开始</span><br><span class="line">def start():</span><br><span class="line">        print &apos;Waiting............&apos;</span><br><span class="line">        url=&apos;http://www.22mm.cc&apos;</span><br><span class="line">        html=get_page(url)</span><br><span class="line">        fenlei_links = get_fenlei_link(url, html)</span><br><span class="line">        for link in fenlei_links:</span><br><span class="line">            pagenum = 0</span><br><span class="line">            while pagenum &lt;= 5:</span><br><span class="line">                pagenum += 1</span><br><span class="line">                if pagenum == 1:</span><br><span class="line">                    path=unicode(str(link).split(&quot;/&quot;)[-2],&apos;utf8&apos;)</span><br><span class="line">                    if not os.path.exists(path):</span><br><span class="line">                        os.mkdir(path)</span><br><span class="line">                    page_index = link+&quot;index&quot;+&quot;.html&quot;</span><br><span class="line">                else:</span><br><span class="line">                    page_index = link+&quot;index_&quot;+str(pagenum)+&quot;.html&quot;</span><br><span class="line">                packlinks = get_pic_pack_link(page_index) #每个分类每页每个图包首地址</span><br><span class="line">                pack_num = 0</span><br><span class="line">                for packlink in packlinks:</span><br><span class="line">                    thread = threading.Thread(target=get_pack_pic,args=(page_index, pagenum, path, packlink, pack_num))</span><br><span class="line">                    thread.setDaemon(True)</span><br><span class="line">                    thread.start()</span><br><span class="line">                    threads.append(thread)</span><br><span class="line">                    pack_num += 1</span><br><span class="line">                    print &quot;开始第%s个线程&quot;%pack_num</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    try:</span><br><span class="line">        start()</span><br><span class="line">        for thread in threads:</span><br><span class="line">            time.sleep(1)</span><br><span class="line">            while thread.join():</span><br><span class="line">                pass</span><br><span class="line">        print &quot;一共下载了%d个图片&quot;%countnum</span><br><span class="line">    except KeyboardInterrupt, e :</span><br><span class="line">        print &quot;一共下载了%d个图片&quot;%countnum</span><br><span class="line">        for thread in threads:</span><br><span class="line">            sys.exit(1)</span><br></pre></td></tr></table></figure></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2016-03-20</span><i class="fa fa-tag"></i><a class="tag" href="/tags/python/" title="python">python </a><a class="tag" href="/tags/spider/" title="spider">spider </a></div></div></div></div><div class="share"><div class="evernote"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></div><div class="weibo"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></div><div class="twitter"><a class="fa fa-twitter" href="http://twitter.com/home?status=,http://yoursite.com/2016/03/20/Note/python/demo/pic_spider/,Harryx,爬图,;"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/2016/03/23/Note/python/demo/socket_chatroom/" title="聊天室">Post Anterior</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/2016/03/19/Note/python/library/coroutine/threading/" title="多线程">Próximo post</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>