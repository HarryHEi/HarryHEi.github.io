<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Ben Wong,wenbinben@gmail.com"><title>torch · Harryx</title><meta name="description" content="文档
TensorsTensors和NumPy的ndarrays(比如由numpy.array()方法创建的实例)类似，并且可以在GPU上加速计算。
所有支持的操作
autogradPyTorch神经网络最核心的是autograd包，提供支持Tensors所有操作方式的自动区分，是一个运行时定义的框"><meta name="keywords" content="Hexo,HTML,Ben,CSS,安卓,android,Linux,linuxdeepin"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">Harryx</a></h3><div class="description"><p>good good study good good play~</p></div></div></div><ul class="social-links"><li><a href="https://twitter.com/harryx520"><i class="fa fa-twitter"></i></a></li><li><a href="http://weibo.com/Booooyakasha"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai </a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="http://7xrooc.com1.z0.glb.clouddn.com/avatar.jpg"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>torch</a></h3></div><div class="post-content"><p><a href="http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html" target="_blank" rel="external">文档</a></p>
<h1 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h1><p>Tensors和NumPy的ndarrays(比如由<code>numpy.array()</code>方法创建的实例)类似，并且可以在GPU上加速计算。</p>
<p><a href="http://pytorch.org/docs/master/torch.html" target="_blank" rel="external">所有支持的操作</a></p>
<h1 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h1><p>PyTorch神经网络最核心的是autograd包，提供支持Tensors所有操作方式的自动区分，是一个运行时定义的框架。</p>
<h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><p><code>autograd.Variable</code>是这个包的核心，包装了一个Tensor，支持Tensor的所有操作。一旦完成计算，可以调用<code>.backward()</code>自动计算梯度。</p>
<p>可以通过<code>.data</code>属性获取原始数据，梯度可以通过<code>.grad</code>属性获取。</p>
<p>还有一个非常重要的类<code>Function</code>，和<code>Variable</code>连接并建立非环状图表，编写完整的计算历史。每个Variable有一个<code>.grad_fn</code>属性保存创建这个Variable的Function的引用，除非是由用户创建的Variables，它们的<code>grad_fn</code>是None。</p>
<p>如果想要计算衍生物，可以调用<code>Variable.backward()</code>，如果<code>Variable</code>是标量，比如一元数据，<code>backward()</code>就不需要指定参数，否则，必须指定<code>grad_output</code>参数匹配Tensor形状。</p>
<p>创建一个Variable<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">In[31]: x=Variable(torch.ones(2,2), requires_grad=True)</div><div class="line">In[32]: x</div><div class="line">Out[32]: </div><div class="line">Variable containing:</div><div class="line"> 1  1</div><div class="line"> 1  1</div><div class="line">[torch.FloatTensor of size 2x2]</div></pre></td></tr></table></figure></p>
<p>进行一些操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">In[33]: y = x * 2</div><div class="line">In[34]: y</div><div class="line">Out[34]: </div><div class="line">Variable containing:</div><div class="line"> 2  2</div><div class="line"> 2  2</div><div class="line">[torch.FloatTensor of size 2x2]</div></pre></td></tr></table></figure></p>
<p>这里y是由计算的结果创建的，所有它具有<code>grad_fn</code>参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">y.grad_fn</div><div class="line">Out[35]: &lt;torch.autograd.function.MulConstantBackward at 0x1eb95684618&gt;</div></pre></td></tr></table></figure></p>
<h2 id="Gradients"><a href="#Gradients" class="headerlink" title="Gradients"></a>Gradients</h2><p>做一些操作，这里out是计算z的平均值:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In[36]: z=y*y*3</div><div class="line">In[37]: z</div><div class="line">Out[37]: </div><div class="line">Variable containing:</div><div class="line"> 12  12</div><div class="line"> 12  12</div><div class="line">[torch.FloatTensor of size 2x2]</div><div class="line">In[38]: out=z.mean()</div><div class="line">In[39]: out</div><div class="line">Out[39]: </div><div class="line">Variable containing:</div><div class="line"> 12</div><div class="line">[torch.FloatTensor of size 1]</div></pre></td></tr></table></figure></p>
<p>回过头调用<code>out.backward()</code>也等同于<code>out.backward(torch.Tensor([1.0]))</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">In[40]: out.backward()</div><div class="line">In[41]: x.grad</div><div class="line">Out[41]: </div><div class="line">Variable containing:</div><div class="line"> 6  6</div><div class="line"> 6  6</div><div class="line">[torch.FloatTensor of size 2x2]</div></pre></td></tr></table></figure>
<p>已知out等于z的平均值，z等于y的平方乘以3，y等于2乘以x，并且当x等于1时z等于12。</p>
<p>列出out和x的表达式: <code>out = 12x^2</code> ，求导得: <code>6</code></p>
<p>与最后的结果相同。</p>
<p>范数:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">In[42]: x=torch.randn(3)</div><div class="line">In[43]: x=Variable(x, requires_grad=True)</div><div class="line">In[44]: y=x*2</div><div class="line">In[45]: while y.data.norm() &lt; 1000:</div><div class="line">   ...:     y = y * 2</div><div class="line">   ...:     </div><div class="line">In[46]: y</div><div class="line">Out[46]: </div><div class="line">Variable containing:</div><div class="line">  949.9868</div><div class="line">-1183.3439</div><div class="line"> 1207.7928</div><div class="line">[torch.FloatTensor of size 3]</div></pre></td></tr></table></figure></p>
<p>调用backward()<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">In[47]: gradients = torch.FloatTensor([0.1, 1.0, 0.0001])</div><div class="line">In[48]: y.backward(gradients)</div><div class="line">In[49]: x.grad</div><div class="line">Out[49]: </div><div class="line">Variable containing:</div><div class="line">  102.4000</div><div class="line"> 1024.0000</div><div class="line">    0.1024</div><div class="line">[torch.FloatTensor of size 3]</div></pre></td></tr></table></figure></p>
<h1 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h1><p>神经网络可以由包<code>torch.nn</code>构建。</p>
<p><code>nn</code>依赖于<code>autograd</code>定义模型、区分。一个<code>nn.Module</code>含有图层以及返回<code>output</code>的方法<code>forward(input)</code>。</p>
<p>例如，一个分类数字图像的网络，简单的前馈网络，接收输入，一个接着一个的输入，然后输出。</p>
<p>神经网络的典型训练过程如下:</p>
<ul>
<li>定义一个有一些可学习参数或权重的网络</li>
<li>迭代输入数据集</li>
<li>通过网络处理输入</li>
<li>计算loss(距离正确的距离)</li>
<li>将gradients传回神经网络的参数</li>
<li>更新神经网络的权重，通常使用一个简单的更新规则:<code>weight = weight - learning_rate * gradient</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">import torch</div><div class="line">from torch.autograd import Variable</div><div class="line">import torch.nn as nn</div><div class="line">import torch.nn.functional as F</div><div class="line"></div><div class="line"></div><div class="line">class HyNet(nn.Module):</div><div class="line">    def __init__(self):</div><div class="line">        super(HyNet, self).__init__()</div><div class="line"></div><div class="line">        self.conv1 = nn.Conv2d(1, 6, 5)</div><div class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</div><div class="line"></div><div class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</div><div class="line">        self.fc2 = nn.Linear(120, 84)</div><div class="line">        self.fc3 = nn.Linear(84, 10)</div><div class="line"></div><div class="line">    def forward(self, x):</div><div class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</div><div class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2)</div><div class="line">        x = x.view(-1, self.num_flat_features(x))</div><div class="line">        x = F.relu(self.fc1(x))</div><div class="line">        x = F.relu(self.fc2(x))</div><div class="line">        x = self.fc3(x)</div><div class="line">        return x</div><div class="line"></div><div class="line">    def num_flat_features(self, x):</div><div class="line">        size = x.size()[1:]</div><div class="line">        num_features = 1</div><div class="line">        for s in size:</div><div class="line">            num_features *= s</div><div class="line">        return num_features</div><div class="line"></div><div class="line">hy_net = HyNet()</div><div class="line">print(hy_net)</div></pre></td></tr></table></figure>
<p>只需要定义<code>forward</code>函数，<code>backward</code>函数会在使用<code>autograd</code>时自动定义，可以使用任何Tensor的操作定义<code>forward</code>函数。</p>
<p>一个模型的可学习参数由<code>.parameters()</code>获取。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">params = list(hy_net.parameters())</div><div class="line">print(len(params))</div><div class="line">print(params[0].size())</div></pre></td></tr></table></figure>
<p>forward的输入和输出都是<code>autograd.Variable</code>。这个网络的预期输入大小是32x32。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">input_ = Variable(torch.randn(1, 1, 32, 32))</div><div class="line">out = hy_net(input_)</div><div class="line">print(out)</div></pre></td></tr></table></figure>
<p>清空gradient buffers，然后调用backward。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hy_net.zero_grad()</div><div class="line">out.backward(torch.randn(1, 10))</div></pre></td></tr></table></figure>
<p><code>torch.nn</code>仅支持小批量的样品输入，不支持单个。</p>
<p>例如，<code>nn.Conv2d</code>将采用4D     Tensor<code>nSamples * nChannels * Height * Width</code>。</p>
<p>如果有个单独样例，使用<code>input.unsqueeze(0)</code>添加虚假的批量纬度。</p>
<h1 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h1><p>loss function拥有两个输入<code>(output, target)</code>，然后计算output和target之间的差值。</p>
<p>在<code>nn</code>包中有一些不同的loss function，一个简单的loss是<code>nn.MSELoss</code>，计算input和target之间的均方误差。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">output = hy_net(input_)</div><div class="line">target = Variable(torch.arange(1, 11))</div><div class="line">criterion = nn.MSELoss()</div><div class="line"></div><div class="line">loss = criterion(output, target)</div><div class="line">print(loss)</div></pre></td></tr></table></figure>
<h1 id="Backprop"><a href="#Backprop" class="headerlink" title="Backprop"></a>Backprop</h1><p>为了反向传播错误，调用<code>loss.backward()</code>，需要清除现有的gradients，否则将累加到现有的gradients。</p>
<p>现在调用<code>loss.backward()</code>，然后查看backward之前和之后的conv1的gradients</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">print(&apos;conv1.bias.grad before backward&apos;)</div><div class="line">print(hy_net.conv1.bia.grad)</div><div class="line"></div><div class="line">loss.backward()</div><div class="line"></div><div class="line">print(&apos;conv1.bias.grad after backward&apos;)</div><div class="line">print(hy_net.conv1.bia.grad)</div></pre></td></tr></table></figure>
<p>输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">conv1.bias.grad before backward</div><div class="line">Variable containing:</div><div class="line"> 0</div><div class="line"> 0</div><div class="line"> 0</div><div class="line"> 0</div><div class="line"> 0</div><div class="line"> 0</div><div class="line">[torch.FloatTensor of size 6]</div><div class="line"></div><div class="line">conv1.bias.grad after backward</div><div class="line">Variable containing:</div><div class="line">1.00000e-02 *</div><div class="line">  1.8583</div><div class="line">  0.0107</div><div class="line">  2.6561</div><div class="line">  1.3492</div><div class="line">  2.4392</div><div class="line">  2.0675</div><div class="line">[torch.FloatTensor of size 6]</div></pre></td></tr></table></figure></p>
<h1 id="更新权重"><a href="#更新权重" class="headerlink" title="更新权重"></a>更新权重</h1><p>最简单的更新方式是Stochastic Gradient Descent(SGD 随机梯度下降)</p>
<p><code>weight = weight - learning_rate * gradient</code></p>
<p>用简单的python代码实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.01</div><div class="line">for f in hy_net.parameters():</div><div class="line">    f.data.sub_(f.grad.data * learning_rate)</div></pre></td></tr></table></figure></p>
<p>然而，当使用神经网络时，需要使用不同的更新规则，比如SGD、Nesterov-SGD、Adam、RMSProp等。<code>torch.optim</code>实现所有这些方法，使用方法如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import torch.optim as optim</div><div class="line"></div><div class="line"># 建立优化器</div><div class="line">optimizer = optim.SGD(hy_net.parameters(), lr=0.01)</div><div class="line"></div><div class="line"># 在训练循环中</div><div class="line">optimizer.zero_grad()  # 清空gradient buffers</div><div class="line">output = hy_net(input_)</div><div class="line">loss = criterion(output, target)</div><div class="line">loss.backward()</div><div class="line">optimizer.step()  # 更新</div></pre></td></tr></table></figure></p>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2018-02-04</span><i class="fa fa-tag"></i><a href="/tags/python/" title="python" class="tag">python </a></div></div></div></div><div class="share"><div class="evernote"><a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"><a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"><a href="http://twitter.com/home?status=,http://yoursite.com/2018/02/04/Note/python/library/torch/,Harryx,torch,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2018/02/07/Note/unity/input/" title="Unity Input" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2018/02/03/Note/tools/cmake/" title="cmake" class="btn">下一篇</a></li></ul></div></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>